Use device: cpu
---
Loading: tokenize
With settings: 
{'model_path': '/home/jiska/Code/ernie/resources/stanfordnlp_resources/nl_alpino_models/nl_alpino_tokenizer.pt', 'pretokenized': True, 'lang': 'nl', 'shorthand': 'nl_alpino', 'mode': 'predict'}
---
Loading: pos
With settings: 
{'model_path': '/home/jiska/Code/ernie/tools/stanfordnlp/lista/stanfordnlp/saved_models/pos/nl_combined_tagger.pt', 'pretrain_path': '/home/jiska/Code/ernie/tools/stanfordnlp/lista/stanfordnlp/saved_models/pos/nl_combined.pretrain.pt', 'lang': 'nl', 'shorthand': 'nl_alpino', 'mode': 'predict'}
---
Loading: lemma
With settings: 
{'model_path': '/home/jiska/Code/ernie/resources/stanfordnlp_resources/nl_alpino_models/nl_alpino_lemmatizer.pt', 'lang': 'nl', 'shorthand': 'nl_alpino', 'mode': 'predict'}
Building an attentional Seq2Seq model...
Using a Bi-LSTM encoder
Using soft attention for LSTM.
Finetune all embeddings.
[Running seq2seq lemmatizer with edit classifier]
---
Loading: depparse
With settings: 
{'model_path': '/home/jiska/Code/ernie/tools/stanfordnlp/lista/stanfordnlp/saved_models/depparse/nl_combined_parser.pt', 'pretrain_path': '/home/jiska/Code/ernie/tools/stanfordnlp/lista/stanfordnlp/saved_models/depparse/nl_combined.pretrain.pt', 'lang': 'nl', 'shorthand': 'nl_alpino', 'mode': 'predict'}
Done loading processors!
---
