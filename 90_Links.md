# Links

Some blogpost and other online resources that can be of interested.

## [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/)

## [Attention in NLP](https://medium.com/@joealato/attention-in-nlp-734c6fa9d983)

## [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
Python notebook going over the Attention is All You Need paper

## [Subword Neural Machine Translation](https://github.com/rsennrich/subword-nmt/blob/master/README.md)
Source code for and documention, also some tools and advice on using byte per encoding (BPE).

## [Visualizing A Neural Machine Translation Model](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

Contains some nice visualizations of attention.
Another post from the same author that can be interesting is [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
